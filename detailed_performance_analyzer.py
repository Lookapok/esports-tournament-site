#!/usr/bin/env python3
"""
æ·±å…¥æ€§èƒ½åˆ†æå·¥å…·
åˆ†æè³‡æ–™åº«æŸ¥è©¢ã€å¿«å–æ•ˆæœã€é é¢çµ„ä»¶è¼‰å…¥æ™‚é–“ç­‰
"""

import requests
import time
import json
import re
from datetime import datetime
from urllib.parse import urljoin

class DetailedPerformanceAnalyzer:
    def __init__(self, base_url="https://winnerstakesall.onrender.com"):
        self.base_url = base_url
        self.session = requests.Session()
        self.analysis_results = {}
        
    def analyze_page_components(self, path, description):
        """åˆ†æé é¢å„çµ„ä»¶çš„è¼‰å…¥æ™‚é–“"""
        url = f"{self.base_url}{path}"
        
        print(f"\nğŸ” åˆ†æé é¢: {description}")
        print("=" * 60)
        
        # 1. æ•´é«”é é¢è¼‰å…¥åˆ†æ
        start_time = time.time()
        try:
            response = self.session.get(url, timeout=30)
            total_load_time = time.time() - start_time
            
            if response.status_code != 200:
                print(f"âŒ é é¢è¼‰å…¥å¤±æ•—: HTTP {response.status_code}")
                return
                
            print(f"âœ… æ•´é«”è¼‰å…¥æ™‚é–“: {total_load_time:.2f}ç§’")
            print(f"ğŸ“„ é é¢å¤§å°: {len(response.content)/1024:.1f} KB")
            print(f"ğŸ“Š ç‹€æ…‹ç¢¼: {response.status_code}")
            
            # 2. åˆ†æéŸ¿æ‡‰æ¨™é ­
            self.analyze_response_headers(response)
            
            # 3. åˆ†æé é¢å…§å®¹
            self.analyze_page_content(response.text, path)
            
            # 4. åˆ†æå¿«å–æ•ˆæœ
            self.analyze_cache_performance(url, description)
            
            self.analysis_results[path] = {
                'description': description,
                'total_load_time': total_load_time,
                'page_size_kb': len(response.content)/1024,
                'status_code': response.status_code,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±æ•—: {e}")
            
    def analyze_response_headers(self, response):
        """åˆ†æéŸ¿æ‡‰æ¨™é ­"""
        print("\nğŸ“‹ éŸ¿æ‡‰æ¨™é ­åˆ†æ:")
        
        important_headers = [
            'Content-Type', 'Content-Length', 'Content-Encoding',
            'Cache-Control', 'ETag', 'Last-Modified',
            'X-Cache', 'Server', 'CF-Cache-Status'
        ]
        
        for header in important_headers:
            if header in response.headers:
                print(f"   {header}: {response.headers[header]}")
                
        # æª¢æŸ¥å¿«å–ç›¸é—œæ¨™é ­
        cache_headers = ['Cache-Control', 'ETag', 'Last-Modified']
        cache_count = sum(1 for h in cache_headers if h in response.headers)
        
        if cache_count > 0:
            print(f"âœ… å¿«å–æ¨™é ­: {cache_count}/{len(cache_headers)} å€‹è¨­å®š")
        else:
            print("âš ï¸  ç„¡å¿«å–æ¨™é ­è¨­å®š")
            
    def analyze_page_content(self, html_content, path):
        """åˆ†æé é¢å…§å®¹çµæ§‹"""
        print("\nğŸ¯ é é¢å…§å®¹åˆ†æ:")
        
        # åˆ†æ HTML å…ƒç´ æ•¸é‡
        img_count = len(re.findall(r'<img[^>]*>', html_content))
        css_count = len(re.findall(r'<link[^>]*rel=["\']stylesheet["\']', html_content))
        js_count = len(re.findall(r'<script[^>]*src=', html_content))
        
        print(f"   ğŸ–¼ï¸  åœ–ç‰‡æ•¸é‡: {img_count}")
        print(f"   ğŸ¨ CSS æª”æ¡ˆ: {css_count}")
        print(f"   âš¡ JS æª”æ¡ˆ: {js_count}")
        
        # æª¢æŸ¥ç‰¹å®šå…§å®¹
        if 'tournament' in path:
            # åˆ†æè³½äº‹ç›¸é—œå…§å®¹
            match_count = len(re.findall(r'class=["\'][^"\']*match[^"\']*["\']', html_content))
            team_count = len(re.findall(r'class=["\'][^"\']*team[^"\']*["\']', html_content))
            
            print(f"   ğŸ† æ¯”è³½å…ƒç´ : {match_count}")
            print(f"   ğŸ‘¥ éšŠä¼å…ƒç´ : {team_count}")
            
        if 'teams' in path:
            # åˆ†æéšŠä¼ç›¸é—œå…§å®¹
            team_card_count = len(re.findall(r'class=["\'][^"\']*card[^"\']*["\']', html_content))
            print(f"   ğŸ“‹ éšŠä¼å¡ç‰‡: {team_card_count}")
            
        # æª¢æŸ¥éŒ¯èª¤æŒ‡ç¤º
        if 'error' in html_content.lower() or '404' in html_content:
            print("âš ï¸  é é¢å¯èƒ½å«æœ‰éŒ¯èª¤å…§å®¹")
            
    def analyze_cache_performance(self, url, description):
        """åˆ†æå¿«å–æ€§èƒ½"""
        print("\nâš¡ å¿«å–æ€§èƒ½åˆ†æ:")
        
        # æ¸…é™¤å¿«å–ï¼Œæ¸¬è©¦å†·è¼‰å…¥
        self.session.cookies.clear()
        cold_times = []
        
        for i in range(3):
            start = time.time()
            response = self.session.get(url, timeout=30)
            cold_time = time.time() - start
            cold_times.append(cold_time)
            time.sleep(1)  # é¿å…è«‹æ±‚éå¿«
            
        # æ¸¬è©¦ç†±è¼‰å…¥ï¼ˆæœ‰å¿«å–ï¼‰
        hot_times = []
        for i in range(3):
            start = time.time()
            response = self.session.get(url, timeout=30)
            hot_time = time.time() - start
            hot_times.append(hot_time)
            time.sleep(0.5)
            
        avg_cold = sum(cold_times) / len(cold_times)
        avg_hot = sum(hot_times) / len(hot_times)
        improvement = ((avg_cold - avg_hot) / avg_cold) * 100 if avg_cold > 0 else 0
        
        print(f"   â„ï¸  å¹³å‡å†·è¼‰å…¥: {avg_cold:.2f}ç§’ (ç¯„åœ: {min(cold_times):.2f}-{max(cold_times):.2f})")
        print(f"   ğŸ”¥ å¹³å‡ç†±è¼‰å…¥: {avg_hot:.2f}ç§’ (ç¯„åœ: {min(hot_times):.2f}-{max(hot_times):.2f})")
        print(f"   ğŸ“ˆ å¿«å–æ”¹å–„: {improvement:.1f}%")
        
        # å¿«å–æ•ˆæœè©•ç´š
        if improvement > 50:
            cache_grade = "å„ªç§€ ğŸ†"
        elif improvement > 20:
            cache_grade = "è‰¯å¥½ âœ…"
        elif improvement > 0:
            cache_grade = "æ™®é€š âš ï¸"
        else:
            cache_grade = "éœ€æ”¹å–„ âŒ"
            
        print(f"   ğŸ¯ å¿«å–è©•ç´š: {cache_grade}")
        
    def analyze_database_patterns(self):
        """åˆ†æå¯èƒ½çš„è³‡æ–™åº«æŸ¥è©¢æ¨¡å¼"""
        print("\nğŸ’¾ è³‡æ–™åº«æŸ¥è©¢æ¨¡å¼åˆ†æ:")
        
        test_cases = [
            ("/", "é¦–é  - è³½äº‹åˆ—è¡¨"),
            ("/tournaments/9/", "è³½äº‹è©³æƒ… - å®Œæ•´è³‡æ–™"),
            ("/tournaments/9/?page=2", "è³½äº‹è©³æƒ… - åˆ†é "),
            ("/teams/", "éšŠä¼åˆ—è¡¨ - ç¬¬ä¸€é "),
            ("/teams/?page=2", "éšŠä¼åˆ—è¡¨ - ç¬¬äºŒé "),
        ]
        
        for path, desc in test_cases:
            print(f"\n   ğŸ” æ¸¬è©¦: {desc}")
            
            # æ¸¬è©¦å¤šæ¬¡è¼‰å…¥çš„ä¸€è‡´æ€§
            times = []
            for i in range(5):
                start = time.time()
                try:
                    response = self.session.get(f"{self.base_url}{path}", timeout=15)
                    load_time = time.time() - start
                    times.append(load_time)
                except:
                    times.append(999)  # æ¨™è¨˜å¤±æ•—
                time.sleep(0.5)
                
            valid_times = [t for t in times if t < 30]
            if valid_times:
                avg_time = sum(valid_times) / len(valid_times)
                std_dev = (sum((t - avg_time) ** 2 for t in valid_times) / len(valid_times)) ** 0.5
                
                print(f"      â±ï¸  å¹³å‡è¼‰å…¥: {avg_time:.2f}ç§’")
                print(f"      ğŸ“Š æ¨™æº–å·®: {std_dev:.2f}ç§’")
                
                if std_dev < 0.5:
                    consistency = "ç©©å®š âœ…"
                elif std_dev < 1.0:
                    consistency = "æ™®é€š âš ï¸"
                else:
                    consistency = "ä¸ç©©å®š âŒ"
                    
                print(f"      ğŸ¯ ç©©å®šæ€§: {consistency}")
                
    def analyze_specific_bottlenecks(self):
        """åˆ†æç‰¹å®šçš„æ€§èƒ½ç“¶é ¸"""
        print("\nğŸš¨ ç“¶é ¸åˆ†æ:")
        
        # æ¸¬è©¦å¤§å‹é é¢ (WTACS S1 - 144å ´æ¯”è³½)
        print("\n   ğŸ† å¤§å‹è³½äº‹åˆ†æ (WTACS S1):")
        
        # æ¸¬è©¦ä¸åŒåˆ†çµ„çš„è¼‰å…¥æ™‚é–“
        group_times = {}
        for page in range(1, 5):  # æ¸¬è©¦å‰4çµ„
            try:
                start = time.time()
                response = self.session.get(
                    f"{self.base_url}/tournaments/9/?page={page}", 
                    timeout=30
                )
                load_time = time.time() - start
                group_times[f"ç¬¬{page}çµ„"] = load_time
                print(f"      ç¬¬{page}çµ„è¼‰å…¥: {load_time:.2f}ç§’")
            except Exception as e:
                print(f"      ç¬¬{page}çµ„å¤±æ•—: {str(e)[:30]}...")
                
        if group_times:
            fastest = min(group_times.values())
            slowest = max(group_times.values())
            print(f"      ğŸ“Š åˆ†çµ„è¼‰å…¥å·®ç•°: {slowest - fastest:.2f}ç§’")
            
    def run_comprehensive_analysis(self):
        """åŸ·è¡Œå…¨é¢æ€§èƒ½åˆ†æ"""
        print("ğŸ”¬ é–‹å§‹æ·±å…¥æ€§èƒ½åˆ†æ...")
        print("=" * 70)
        
        # ä¸»è¦é é¢åˆ†æ
        pages_to_analyze = [
            ("/", "é¦–é /è³½äº‹åˆ—è¡¨"),
            ("/tournaments/9/", "WTACS S1 è³½äº‹è©³æƒ… (Açµ„)"),
            ("/tournaments/9/?page=2", "WTACS S1 è³½äº‹è©³æƒ… (Bçµ„)"),
            ("/teams/", "éšŠä¼åˆ—è¡¨"),
        ]
        
        for path, description in pages_to_analyze:
            self.analyze_page_components(path, description)
            
        # è³‡æ–™åº«æŸ¥è©¢æ¨¡å¼åˆ†æ
        self.analyze_database_patterns()
        
        # ç“¶é ¸åˆ†æ
        self.analyze_specific_bottlenecks()
        
        # ç”Ÿæˆåˆ†æå ±å‘Š
        self.generate_analysis_report()
        
    def generate_analysis_report(self):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“ˆ æ·±å…¥æ€§èƒ½åˆ†æå ±å‘Š")
        print("=" * 70)
        
        if self.analysis_results:
            total_pages = len(self.analysis_results)
            avg_load_time = sum(r['total_load_time'] for r in self.analysis_results.values()) / total_pages
            avg_page_size = sum(r['page_size_kb'] for r in self.analysis_results.values()) / total_pages
            
            print(f"ğŸ“Š æ•´é«”çµ±è¨ˆ:")
            print(f"   åˆ†æé é¢æ•¸: {total_pages}")
            print(f"   å¹³å‡è¼‰å…¥æ™‚é–“: {avg_load_time:.2f}ç§’")
            print(f"   å¹³å‡é é¢å¤§å°: {avg_page_size:.1f} KB")
            
            # æ€§èƒ½åˆ†ç´š
            if avg_load_time < 2.0:
                grade = "å„ªç§€ ğŸ†"
                recommendations = ["ç¶­æŒç•¶å‰å„ªåŒ–ç­–ç•¥", "è€ƒæ…®é€²ä¸€æ­¥çš„å‰ç«¯å„ªåŒ–"]
            elif avg_load_time < 5.0:
                grade = "è‰¯å¥½ âœ…"
                recommendations = ["å„ªåŒ–å¤§å‹é é¢è¼‰å…¥", "å¢å¼·å¿«å–ç­–ç•¥", "è€ƒæ…® CDN"]
            elif avg_load_time < 10.0:
                grade = "æ™®é€š âš ï¸"
                recommendations = ["æ€¥éœ€è³‡æ–™åº«å„ªåŒ–", "å¯¦æ–½æ›´ç©æ¥µçš„å¿«å–", "æ¸›å°‘é é¢è¤‡é›œåº¦"]
            else:
                grade = "éœ€æ”¹å–„ âŒ"
                recommendations = ["å…¨é¢æª¢æŸ¥è³‡æ–™åº«æŸ¥è©¢", "é‡æ–°è¨­è¨ˆé é¢æ¶æ§‹", "è€ƒæ…®åˆ†æ•£å¼å¿«å–"]
                
            print(f"\nğŸ¯ æ•´é«”æ€§èƒ½è©•ç´š: {grade}")
            
            print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
                
        # ä¿å­˜è©³ç´°å ±å‘Š
        with open('detailed_performance_analysis.json', 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_pages_analyzed': len(self.analysis_results),
                    'average_load_time': avg_load_time if self.analysis_results else 0,
                    'average_page_size_kb': avg_page_size if self.analysis_results else 0,
                },
                'detailed_results': self.analysis_results
            }, f, ensure_ascii=False, indent=2)
            
        print(f"\nğŸ“‹ è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: detailed_performance_analysis.json")

if __name__ == "__main__":
    analyzer = DetailedPerformanceAnalyzer()
    analyzer.run_comprehensive_analysis()
